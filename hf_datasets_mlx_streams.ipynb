{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Use HuggingFace Datasets With MLX Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "\n",
    "# Buffer always return numpy arrays (bug?)\n",
    "buff = dx.buffer_from_vector([{\"x\": i} for i in range(10)])\n",
    "print(type(buff[0]['x']))\n",
    "\n",
    "buff = buff.key_transform(\"x\", lambda x: mx.ones(3), output_key=\"o\")\n",
    "print(type(buff[0]['x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image has shape  (784,)\n",
      "The image should display a  5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from typing import Dict, Any\n",
    "from types import SimpleNamespace\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import mlx.core as mx\n",
    "import mlx.data as dx\n",
    "\n",
    "# Convert the Hugging Face dataset to the custom format\n",
    "def huggingface_to_bytes_dicts(dataset):    \n",
    "    return [{\"image\": image.tobytes(), \"label\": label}\n",
    "            for label, image in zip(dataset['label'], dataset['image'])]\n",
    "\n",
    "# Convert the Hugging Face dataset to a stream of batches\n",
    "def hf_dataset_to_mlx_stream(dataset, shuffle=False, bs=256, prefetch_size=4, num_threads=4):\n",
    "    buffer = dx.buffer_from_vector(huggingface_to_bytes_dicts(dataset))\n",
    "    if shuffle:\n",
    "        buffer = buffer.shuffle()\n",
    "    \n",
    "    return (\n",
    "        buffer\n",
    "        .to_stream()\n",
    "        .key_transform(\"image\", lambda x: x.astype(\"float32\").reshape(-1) / 255) # flat tensor because we gonna use a MLP model      \n",
    "        .batch(bs)\n",
    "        .prefetch(prefetch_size, num_threads) # fetch batches in background threads\n",
    "    )\n",
    "\n",
    "# Load the MNIST dataset from Hugging Face\n",
    "ds = load_dataset(\"ylecun/mnist\")\n",
    "\n",
    "# Transform the dataset to streams\n",
    "train_stream = hf_dataset_to_mlx_stream(ds['train'], shuffle=True)\n",
    "\n",
    "# Iterate on the batches\n",
    "train_stream.reset()\n",
    "for batch_counter, batch in enumerate(train_stream):\n",
    "    (X, y) = mx.array(batch['image']), mx.array(batch['label'])\n",
    "\n",
    "    print('The image has shape ', X[0].shape)    \n",
    "    print('The image should display a ', y[0].item())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 60000\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Dataset To MLX Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Config To Keep Config/HyperParams Organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = SimpleNamespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset and Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use only the training set for now, splitting it into two parts:\n",
    "- Training Set: 80% for training our model.\n",
    "- Validation Set: 20% for checking the model’s accuracy at each epoch.\n",
    "\n",
    "We’ll keep the test set aside for the final evaluation to see how our model performs on completely unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ylecun/mnist\")\n",
    "split_ds = ds['train'].train_test_split(test_size=0.2)\n",
    "ds = {\n",
    "    'train': split_ds['train'],\n",
    "    'val': split_ds['test'],\n",
    "    'test': ds['test']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.PngImagePlugin.PngImageFile"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds['train']['image'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our mission is to turn this into a mx.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Hugging Face Dataset To MLX Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mlx.data as dx\n",
    "\n",
    "# Convert the content of the dataset into numpy arrays\n",
    "def huggingface_to_bytes_dicts(dataset):    \n",
    "    return [{\"image\": image.tobytes(), \"label\": label}\n",
    "            for label, image in zip(dataset['label'], dataset['image'])]\n",
    "\n",
    "# Convert the Hugging Face dataset to a stream of batches\n",
    "def hf_dataset_to_mlx_stream(dataset, shuffle=False, bs=256, prefetch_size=4, num_threads=4):\n",
    "    bytes_dicts = huggingface_to_bytes_dicts(dataset)\n",
    "\n",
    "    # This might look tedious but these little assert will save you a lot of time\n",
    "    assert type(bytes_dicts) == list, \"Processed data should be a list\"\n",
    "    assert len(bytes_dicts) == len(dataset), \"Output length should match input length\"\n",
    "    assert type(bytes_dicts[0]) == dict, \"Each item should be a dictionary\"\n",
    "    assert 'image' in bytes_dicts[0] and 'label' in bytes_dicts[0], \"Each dict should have 'image' and 'label' keys\"\n",
    "    assert type(bytes_dicts[0]['image']) == bytes, f\"{type(bytes_dicts[0]['image'])} should be an array of bytes\"\n",
    "\n",
    "    buffer = dx.buffer_from_vector(bytes_dicts)\n",
    "    if shuffle:\n",
    "        buffer = buffer.shuffle()    \n",
    "    \n",
    "    return (\n",
    "        buffer\n",
    "        .to_stream()\n",
    "        .key_transform(\"image\", lambda x: x.astype(\"float32\").reshape(-1) / 255) # flat tensor because we gonna use a MLP model      \n",
    "        .batch(bs)\n",
    "        .prefetch(prefetch_size, num_threads) # fetch batches in background threads\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping Params Oganized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "cfg = SimpleNamespace()\n",
    "# These are more Loading Param\n",
    "cfg.prefetch = 4\n",
    "cfg.num_threads = 8\n",
    "cfg.batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stream()"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stream = hf_dataset_to_mlx_stream(ds['train'],\n",
    "                                        shuffle=True,\n",
    "                                        bs=cfg.batch_size,\n",
    "                                        prefetch_size=cfg.prefetch,\n",
    "                                        num_threads=cfg.num_threads)\n",
    "train_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally We Can Iterate On The Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image should display a  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch time: 0.11091 (s)\n"
     ]
    }
   ],
   "source": [
    "# helper to display an image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def np_show_img(array, title=''):\n",
    "    if array.ndim == 1:  # If rank 1, reshape to 2D\n",
    "        array = array.reshape(28, 28)\n",
    "    plt.imshow(array, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def one_epoch():\n",
    "    train_stream.reset()\n",
    "    for batch in train_stream:    \n",
    "        (X, y) = mx.array(batch['image']), mx.array(batch['label'])\n",
    "        \n",
    "        # we want a flat tensor (batch_size,pixels) since we gonna use a MLP instead of a CNN\n",
    "        assert X.ndim == 2, X.shape\n",
    "        assert X.shape == (cfg.batch_size, 784), X.shape\n",
    "        assert X.dtype == mx.float32, X.dtype\n",
    "        assert y.shape == (cfg.batch_size,), y.shape\n",
    "            \n",
    "        print('The image should display a ', y[0].item())\n",
    "        np_show_img(X[0])\n",
    "        break # USE OPTIMIZER HERE\n",
    "\n",
    "# ADD YOUR EPOCH LOOP, ETC HERE\n",
    "tic = time.perf_counter()\n",
    "one_epoch()\n",
    "toc = time.perf_counter()\n",
    "print(f\"epoch time: {toc - tic:.5f} (s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
