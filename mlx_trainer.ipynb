{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image has shape  (784,)\n",
      "The image should display a  5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from typing import Dict, Any\n",
    "from types import SimpleNamespace\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import mlx.core as mx\n",
    "import mlx.data as dx\n",
    "\n",
    "# Convert the Hugging Face dataset to the custom format\n",
    "def huggingface_to_array_of_dict(dataset):    \n",
    "    return [{\"image\": np.array(image).copy(), \"label\": label}\n",
    "            for label, image in zip(dataset['label'], dataset['image'])]\n",
    "\n",
    "# Convert the Hugging Face dataset to a stream of batches\n",
    "def hf_dataset_to_mlx_stream(dataset, cfg: Dict, shuffle=False):\n",
    "    buffer = dx.buffer_from_vector(huggingface_to_array_of_dict(dataset))\n",
    "    if shuffle:\n",
    "        buffer = buffer.shuffle()\n",
    "    \n",
    "    return (\n",
    "        buffer\n",
    "        .to_stream()\n",
    "        .key_transform(\"image\", lambda x: x.astype(\"float32\").reshape(-1) / 255)        \n",
    "        .batch(cfg.batch_size)  # the value doesn't matter for now\n",
    "        .prefetch(cfg.prefetch_size, cfg.num_threads) # fetch batches in background threads\n",
    "    )\n",
    "\n",
    "# Load the MNIST dataset from Hugging Face\n",
    "# We’ll use only the training set for now, splitting it into two parts:\n",
    "# - Training Set: 80% for training our model.\n",
    "# - Validation Set: 20% for checking the model’s accuracy at each epoch.\n",
    "# - Test Set: kept for totally unseen data at the final stage\n",
    "ds = load_dataset(\"ylecun/mnist\")\n",
    "split_ds = ds['train'].train_test_split(test_size=0.2)\n",
    "ds = {\n",
    "    'train': split_ds['train'],\n",
    "    'val': split_ds['test'],\n",
    "    'test': ds['test']\n",
    "}\n",
    "\n",
    "cfg = SimpleNamespace()\n",
    "cfg.batch_size = 4\n",
    "cfg.prefetch_size = 4\n",
    "cfg.num_threads = 4\n",
    "\n",
    "# Transform the dataset to streams\n",
    "train_stream = hf_dataset_to_mlx_stream(ds['train'], cfg, shuffle=True)\n",
    "\n",
    "# Iterate on the batches\n",
    "train_stream.reset()\n",
    "for batch_counter, batch in enumerate(train_stream):\n",
    "    (X, y) = mx.array(batch['image']), mx.array(batch['label'])\n",
    "\n",
    "    print('The image has shape ', X[0].shape)    \n",
    "    print('The image should display a ', y[0].item())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs):\n",
    "        super().__init__()\n",
    "        self.layers = [\n",
    "            nn.Linear(n_inputs, n_hidden),\n",
    "            nn.Linear(n_hidden, n_outputs)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = nn.relu(layer(x))\n",
    "        # we don't do ReLU for last layer\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Helper Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Class (Not That Interesting Just Move On)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import mlx.core as mx\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self):\n",
    "        self.epochs = {}\n",
    "        self.metrics_df = pd.DataFrame(columns=[\"epoch\", \"tr_loss\", \"val_loss\", \"accuracy\", \"samples/s\", \"time\"])\n",
    "\n",
    "    def start_epoch(self, epoch):\n",
    "        self.start_epoch_timer = time.perf_counter()\n",
    "        self.epochs[epoch] = {}\n",
    "\n",
    "    def start_batch(self, epoch):\n",
    "        self.start_batch_timer = time.perf_counter()\n",
    "        self.epochs[epoch]['batch_losses'] = []\n",
    "        self.epochs[epoch]['batch_samples_per_sec'] = []\n",
    "\n",
    "    def finish_batch(self, epoch, batch_size, loss):\n",
    "        self.epochs[epoch]['batch_losses'].append(loss.item())\n",
    "        self.epochs[epoch]['batch_samples_per_sec'].append(batch_size / (time.perf_counter() - self.start_batch_timer))\n",
    "        # print(self.epochs[epoch]['batch_samples_per_sec'])\n",
    "\n",
    "    def finish_epoch(self, epoch, val_acc, val_loss):\n",
    "        self.epochs[epoch]['tr_loss'] = mx.mean(mx.array(self.epochs[epoch]['batch_losses']))\n",
    "        self.epochs[epoch]['val_loss'] = val_loss\n",
    "        self.epochs[epoch]['accuracy'] = val_acc\n",
    "        self.epochs[epoch]['throughput'] = mx.mean(mx.array(self.epochs[epoch]['batch_samples_per_sec']))\n",
    "        self.epochs[epoch]['time'] = time.perf_counter() - self.start_epoch_timer\n",
    "\n",
    "    def print_metrics(self):\n",
    "        epoch = len(self.epochs) - 1\n",
    "        data = self.epochs[epoch]\n",
    "        data = {\n",
    "            'epoch': epoch,\n",
    "            'tr_loss': f\"{data['tr_loss'].item():.6f}\",\n",
    "            'val_loss': f\"{data['val_loss'].item():.6f}\",\n",
    "            'accuracy': f\"{data['accuracy'].item():.6f}\",\n",
    "            'samples/s': f\"{int(data['throughput'].item())}\",\n",
    "            'time': f\"{int(divmod(data['time'], 60)[0]):02}:{int(divmod(data['time'], 60)[1]):02}\"\n",
    "        }\n",
    "        new_row = pd.DataFrame([data])\n",
    "        self.metrics_df = pd.concat([self.metrics_df, new_row], ignore_index=True)\n",
    "        clear_output(wait=True)\n",
    "        display(HTML(self.metrics_df.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learner Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "from functools import partial\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, cfg, train_stream, val_stream, loss_fn, accuracy_fn):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.cfg = cfg\n",
    "        self.train_stream = train_stream\n",
    "        self.val_stream = val_stream\n",
    "        self.loss_fn = loss_fn\n",
    "        self.accuracy_fn = accuracy_fn\n",
    "\n",
    "    def one_epoch(self, epoch):\n",
    "        @partial(mx.compile, inputs=self.model.state, outputs=self.model.state)\n",
    "        def step(X, y):\n",
    "            loss, grads = nn.value_and_grad(self.model, self.loss_fn)(self.model, X, y)\n",
    "            # this version doesn't work, seems like we need to pass the model as an argument\n",
    "            # and do the forward pass inside the loss function\n",
    "            # preds = self.model(X)\n",
    "            # loss, grads = nn.value_and_grad(self.model, self.loss_fn)(preds, y)\n",
    "            self.optimizer.update(self.model, grads)\n",
    "            return loss # we can't do .item() here because we use mx.compile\n",
    "\n",
    "        for batch in self.train_stream:\n",
    "            X = mx.array(batch[\"image\"])\n",
    "            y = mx.array(batch[\"label\"])\n",
    "            self.metrics.start_batch(epoch)            \n",
    "\n",
    "            # interesting stuff\n",
    "            loss = step(X, y)\n",
    "            mx.eval(self.model.state, self.optimizer.state)\n",
    "            \n",
    "            self.metrics.finish_batch(epoch, X.shape[0], loss)    \n",
    "        \n",
    "    def validate_epoch(self) -> Tuple[mx.array, mx.array]: #)returns an array with a single scalar\n",
    "        accs = []\n",
    "        losses = []\n",
    "        for batch in self.val_stream:            \n",
    "            X = mx.array(batch[\"image\"])\n",
    "            y = mx.array(batch[\"label\"])\n",
    "            preds = self.model(X)\n",
    "            # acc\n",
    "            acc = self.accuracy_fn(preds, y)\n",
    "            accs.append(acc.item()) # use .item() to get a scalar\n",
    "            # loss\n",
    "            preds = self.model(X)\n",
    "            loss = self.loss_fn(self.model, X, y)\n",
    "            losses.append(loss.item())\n",
    "        acc =  mx.mean(mx.array(accs))\n",
    "        assert acc.ndim == 0, acc.shape\n",
    "        loss =  mx.mean(mx.array(loss))\n",
    "        assert loss.ndim == 0, loss.shape\n",
    "        return acc, loss\n",
    "\n",
    "\n",
    "    def fit(self, cfg: Dict[str, Any]):\n",
    "        self.metrics = Metrics()\n",
    "\n",
    "        for epoch in range(cfg.epochs):\n",
    "            self.train_stream.reset()\n",
    "            self.val_stream.reset()\n",
    "\n",
    "            self.metrics.start_epoch(epoch)\n",
    "\n",
    "            self.one_epoch(epoch)\n",
    "            val_acc, val_loss = self.validate_epoch()\n",
    "            self.metrics.finish_epoch(epoch, val_acc, val_loss)\n",
    "            self.metrics.print_metrics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Blueprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>epoch</th>\n",
       "      <th>tr_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>samples/s</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.902779</td>\n",
       "      <td>1.891594</td>\n",
       "      <td>0.656822</td>\n",
       "      <td>29188</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.461295</td>\n",
       "      <td>1.391238</td>\n",
       "      <td>0.750693</td>\n",
       "      <td>17873</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.967882</td>\n",
       "      <td>0.766786</td>\n",
       "      <td>0.806698</td>\n",
       "      <td>77172</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.689295</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>0.831757</td>\n",
       "      <td>118270</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.585121</td>\n",
       "      <td>0.683638</td>\n",
       "      <td>0.848779</td>\n",
       "      <td>32366</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.268904</td>\n",
       "      <td>0.641596</td>\n",
       "      <td>0.859907</td>\n",
       "      <td>11604</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.530931</td>\n",
       "      <td>0.327467</td>\n",
       "      <td>0.872267</td>\n",
       "      <td>78083</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.476316</td>\n",
       "      <td>0.518387</td>\n",
       "      <td>0.880370</td>\n",
       "      <td>95709</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.426353</td>\n",
       "      <td>0.717427</td>\n",
       "      <td>0.879537</td>\n",
       "      <td>88518</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.391617</td>\n",
       "      <td>0.377124</td>\n",
       "      <td>0.884126</td>\n",
       "      <td>61121</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlx.optimizers as optim\n",
    "\n",
    "# download dataset\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# Load the MNIST dataset from Hugging Face\n",
    "# We’ll use only the training set for now, splitting it into two parts:\n",
    "# - Training Set: 80% for training our model.\n",
    "# - Validation Set: 20% for checking the model’s accuracy at each epoch.\n",
    "# - Test Set: kept for totally unseen data at the final stage\n",
    "# ds = load_dataset(\"ylecun/mnist\")\n",
    "# split_ds = ds['train'].train_test_split(test_size=0.2)\n",
    "# ds = {\n",
    "#     'train': split_ds['train'],\n",
    "#     'val': split_ds['test'],\n",
    "#     'test': ds['test']\n",
    "# }\n",
    "\n",
    "# Get the streams (dataloaders in pytorch)\n",
    "# Eventually modify hf_dataset_to_mlx_stream to be compatible with the shapes/transforms you need\n",
    "train_stream = hf_dataset_to_mlx_stream(ds['train'], cfg, shuffle=True)\n",
    "val_stream = hf_dataset_to_mlx_stream(ds['val'], cfg, shuffle=False)\n",
    "\n",
    "# loss function for SGD\n",
    "# @mx.compile\n",
    "def loss_fn(model: nn.Module, X: mx.array, y: mx.array):\n",
    "    assert X.shape[0] == y.shape[0], (X.shape, y.shape)\n",
    "    assert X.shape[1] == n_inputs, X.shape    \n",
    "    preds = model(X)\n",
    "    assert preds.shape == (X.shape[0], n_classes), preds.shape\n",
    "    return nn.losses.cross_entropy(preds, y, reduction=\"mean\")\n",
    "\n",
    "# accuracy function for human metric\n",
    "@mx.compile\n",
    "def accuracy_fn(preds: mx.array, y: mx.array) -> mx.array:        \n",
    "    assert preds.shape[0] == y.shape[0], (preds.shape, y.shape)    \n",
    "    r = mx.mean(mx.argmax(preds, axis=1) == y) \n",
    "    assert r.ndim == 0, r.shape\n",
    "    return r # we can't do .item() here because we use mx.compile\n",
    "\n",
    "# HYPER PARAMS!\n",
    "# Loading Param\n",
    "cfg.prefetch = 4\n",
    "cfg.num_threads = 8\n",
    "cfg.batch_size = 256\n",
    "# Training Param\n",
    "cfg.lr = 1e-2\n",
    "cfg.epochs = 10\n",
    "\n",
    "# define shapes\n",
    "train_stream.reset()\n",
    "one_batch = next(iter(train_stream))\n",
    "one_batch['image'].shape[-1]\n",
    "n_inputs = one_batch['image'].shape[-1]\n",
    "n_hidden = 50\n",
    "n_classes = len(ds['val'].features['label'].names)\n",
    "\n",
    "# The training execution\n",
    "model = MLP(n_inputs, n_hidden, n_classes)\n",
    "optimizer = optim.SGD(learning_rate=cfg.lr)\n",
    "trainer = Trainer(model, optimizer, cfg, train_stream, val_stream, loss_fn=loss_fn, accuracy_fn=accuracy_fn)\n",
    "trainer.fit(cfg)\n",
    "\n",
    "# sometimes samples/s might not make sense, samples/s might be higher than the whole dataset\n",
    "# don't worry, it's just because is an extrapolation per sec\n",
    "# probably your epoch took less than a second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
