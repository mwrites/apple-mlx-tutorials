{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use PyTorch DataLoaders with MLX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚠️  Why You Probably Don't Want To Use This Solution\n",
    "\n",
    "What you are probably looking for is [How To Use HuggingFace Datasets With MLX Streams](./hf_datasets_mlx_streams.ipynb).\n",
    "\n",
    "However, if you need a quick port of your existing PyTorch setup and are not concerned about slower training speeds, read on.\n",
    "\n",
    "__Warning__: Using PyTorch dataloaders means data loading will be as slow as a standard PyTorch on mac solution! You won’t fully benefit from MLX’s performance improvements during training because we have to convert tensors to MLX arrays. The advantage is that you can maintain the standard industry workflow and still enjoy MLX’s benefits during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLDR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a custom collate function with `DataLoader` to convert the data to mx.arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# \n",
    "# def mlx_collate_fn(batch):\n",
    "#     images, labels = zip(*batch)\n",
    "#     images = mx.array([mx.array(image.numpy()) for image in images])\n",
    "#     labels = mx.array(labels)\n",
    "#     return images, labels\n",
    "\n",
    "# DataLoader(dataset_class,batch_size=batch_size,shuffle=shuffle, collate_fn=mlx_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step By Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide walks you through loading a dataset with Hugging Face, batching and collating the data using PyTorch’s DataLoader, and converting the tensors to MX arrays for neural network input, here are the steps:\n",
    "\n",
    "1. Load Dataset from Hugging Face\n",
    "2. Define Your Transforms\n",
    "3. Leverage PyTorch DataLoaders with a Custom Collate Function\n",
    "4. Verify the Setup\n",
    "5. Full Snippet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Install PyTorch if needed](https://pytorch.org/get-started/locally/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Hugging Face Datasets Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/cactus/miniconda3/lib/python3.12/site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in /Users/cactus/miniconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/cactus/miniconda3/lib/python3.12/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/cactus/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /Users/cactus/miniconda3/lib/python3.12/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/cactus/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /Users/cactus/miniconda3/lib/python3.12/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: packaging in /Users/cactus/miniconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/cactus/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlx\n",
    "import mlx.core as mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use HuggingFace's `datasets` library to load the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_name = \"ylecun/mnist\"\n",
    "ds = load_dataset(ds_name)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use only the training set for now, splitting it into two parts:\n",
    "- Training Set: 80% for training our model.\n",
    "- Validation Set: 20% for checking the model’s accuracy at each epoch.\n",
    "\n",
    "We’ll keep the test set aside for the final evaluation to see how our model performs on completely unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 48000\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "# just some renaming to stay consistent with what I just said\n",
    "ds['val'] = ds['test']\n",
    "del ds['test']\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+tbQPDGteKLuW10XT5byaKMyOEwAqj1JIH0HU9q1o/hj43lgMy+GNRCjPDRbW4/2Tz+lczdWtxY3MltdwSwTxnDxSoVZT7g8ioa9m0/XL3wH8BdN1LQ9lvqWsag6yXSxhmCIXGDkEZ+TgehPevNNR8YeJdXDLqGvalcI3WN7lynTH3c4/SsVmLHLEknuaFUuwVRkk4Ar2H4eWOtQ6FrOn+LLAJ4OtIpJZ01GLYUnxx5DEZD59OOfUjPjxxk4BA7ZpKVWKsGU4IOQa3fEHjTxH4qWJNb1ae7jixsjbCoCO+1QAT74zWDRX/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5ElEQVR4AWNgGEqAEdmxTLUNUycxWJgAxZbfeI8sA2RX/0WA660MDMg6vTYwg1T/Zri299l6hu8vkbXqPvn7987ptDR2ZEEYe/rfv9v4YRwwzYTM+133EZnLgJBkEmH4fQZFDonD9vfvfCQuiInQCeQ8R5NEcFlf/MVpKgPD9L/fvRFq0VgyZ37/mi2CJojghv79OwvBQ2Mxuz7/oo4khuLa/wwi+28iSbIgsRlUdzCcQ+bD2ApWDAxKd/7+RTYVJsegdqt3/q+/L82RoxAhCYrmnbJwPgpDbOPfPY4oDkCRJpUDANjxTIDc9KHIAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a sample image and label from the training set\n",
    "sample = ds['train'][0]\n",
    "sample['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the above image is supposed to be a 5\n"
     ]
    }
   ],
   "source": [
    "print('the above image is supposed to be a', sample['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define Your Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we got a PILImage which is not useful for our model, so we need to define a `transform` to go from PIL To Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([784]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    lambda x: x.view(-1)  # Flatten to 1D array of 784 pixels\n",
    "])\n",
    "\n",
    "# Apply transformation to the dataset\n",
    "def transform_dataset(batch):\n",
    "    batch['image'] = [transform(image) for image in batch['image']]\n",
    "    return batch\n",
    "\n",
    "# Apply the transformation to the dataset\n",
    "transformed = ds.with_transform(transform_dataset)\n",
    "\n",
    "# Example of accessing transformed data\n",
    "type(transformed['train'][0]['image']), transformed['train'][0]['image'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that this part `lambda x: x.view(-1)` may vary depending on the requirements of your neural network. A flat tensor is suitable for a multi-layer perceptron, but for a convolutional network, you might need a 3-dimensional tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that we can still display the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAIR0lEQVR4nO3csauWZQPH8fvxnJqcwhZxiCAnoeUMzW2CNIgO0Rou/QURiQT+BQYNEaiDNEQmKhUUDUFT6Fbo0iAUgmBOUgZP0/ull1foue7Xx/Oon898ftzXcDhfr8FrsVwulxMATNO0Z7cPAMDmEAUAIgoARBQAiCgAEFEAIKIAQEQBgGyv+oOLxWKd5wBgzVb5v8puCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAbO/2AeDf7Nkz/m+X999/f3hz6tSp4c2HH344vDlz5szwZq7XXntteLOzs7OGk+yuCxcuDG9u3LgxvLl79+7wZtO4KQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgCyWy+VypR9cLNZ9Fnio9957b3jzwQcfrOEkPEtu3rw5vPn8889nfWvO7/gcq/y5d1MAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQDilVQem8OHD8/aXbp0aXiztbU161tPmwcPHjyW7/z000/Dm2+//XbWt3799dfhzcWLF2d9a9T9+/dn7W7fvv2IT/JwXkkFYIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBAPIjHLIcOHRrefPnll7O+tX///lm7Ub/88svw5u7du8Objz/+eHgz17lz54Y3f/zxxxpOwibwIB4AQ0QBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQCyvdsH4Mn0zjvvDG8e18N20zRNX3/99fDmzTffHN7cu3dveAObzE0BgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgDEg3hsvAcPHgxvTp48ObzxuB24KQDwD6IAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQDilVSmPXvG/22wb9++NZzk4ea8kvrjjz+u4STw9HNTACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIA8SAe0/b2+K/B0aNH13CSh/vss88e27fgWeemAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUA4kE8Nt5vv/2220eAZ4abAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAyGK5XC5X+sHFYt1nYZc899xzw5tbt24Nb1588cXhzTRN0/Xr14c3Ozs7s74FT7NV/ty7KQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgHgQj1k++uij4c2JEydmfevPP/8c3hw7dmx4c/Xq1eENPEk8iAfAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgDxSiqzHDhwYHjzxRdfzPrWq6++OrxZ8df6v5w7d2548+677w5v7ty5M7yBR8ErqQAMEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgH8dh4x48fH958+umnazjJ//rkk0+GNydOnFjDSeDfeRAPgCGiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIA8SAeG29ra2t48/rrrw9vzp8/P7zZu3fv8GZnZ2d4M03TdOPGjVk7+A8P4gEwRBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACDbu30A+Dcrvtn4f9u3b9/w5quvvhreeNiOTeamAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUA4kE8Nt4rr7wyvJnzUN0c165deyzfgcfFTQGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIhXUjfUSy+9NGu3f//+4c0PP/ww61ujXn755Vm7q1evPuKTPDoXLlzY7SPAI+WmAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUA4kG8DfX888/P2p09e3Z4c/ny5eHNCy+8MLx56623hjfTNE1bW1vDmzt37gxv3njjjeHNzZs3hzewydwUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBAFsvlcrnSDy4W6z4L/3Dw4MFZu59//vkRn2T3ffPNN8Obt99+e3hz69at4Q08SVb5c++mAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAsr3bB+Dhfv/991m7K1euDG+OHDkyvPnuu++GN6dPnx7eTNM0ff/998Obv/76a9a34FnnpgBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAGSxXC6XK/3gYrHuswCwRqv8uXdTACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACDbq/7gcrlc5zkA2ABuCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoA5G/s8Qf2JvkXoAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# helper to display an image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def np_show_img(array, title=''):\n",
    "    if array.ndim == 1:  # If rank 1, reshape to 2D\n",
    "        array = array.reshape(28, 28)\n",
    "    plt.imshow(array, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "np_show_img(transformed['train'][0]['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Leverage PyTorch DataLoaders with a Custom Collate Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we successfully applied the transform, but in practice we don't apply transforms on the dataset all at once. We do that in batches, load the stuff in parallel, cache the results, etc.\n",
    "\n",
    "Let's not bother rewriting all this stuff!\n",
    "\n",
    "Instead we can leverage `DataLoader` and `Dataset` classes from the PyTorch utility library and apply a custom collate function to collect our mlx arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import mlx.core as mx\n",
    "\n",
    "# Custom Dataset class to match what the data structure we got from hugging face\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx]['image']\n",
    "        label = self.dataset[idx]['label']\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 👇 The important part\n",
    "def mlx_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = mx.array([mx.array(image.numpy()) for image in images])\n",
    "    labels = mx.array(labels)\n",
    "    return images, labels\n",
    "\n",
    "# Create DataLoaders with the custom collate function\n",
    "def create_dataloader(dataset, transform=None, batch_size=32, shuffle=False):\n",
    "    return DataLoader(MNISTDataset(dataset, transform=transform), # 👈 here we pass the transform we defined earlier\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      collate_fn=mlx_collate_fn)\n",
    "\n",
    "\n",
    "train_loader = create_dataloader(ds['train'], transform=transform, batch_size=batch_size, shuffle=True)\n",
    "val_loader = create_dataloader(ds['val'], batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Verify the Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: <class 'mlx.core.array'> (32, 784)\n",
      "labels: <class 'mlx.core.array'> (32,)\n",
      "The image should display a  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for inputs, labels in train_loader:\n",
    "    print('inputs:', type(inputs), inputs.shape)\n",
    "    print('labels:', type(labels), labels.shape)\n",
    "   \n",
    "    print('The image should display a ', labels[0].item())\n",
    "    np_show_img(inputs[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Full Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: <class 'mlx.core.array'> (32, 784)\n",
      "labels: <class 'mlx.core.array'> (32,)\n",
      "The image should display a  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import mlx\n",
    "import mlx.core as mx\n",
    "\n",
    "# just a helper to show img\n",
    "def np_show_img(array, title=''):\n",
    "    if array.ndim == 1:  # If rank 1, reshape to 2D\n",
    "        array = array.reshape(28, 28)\n",
    "    plt.imshow(array, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    lambda x: x.view(-1)  # Flatten to 1D array of 784 pixels\n",
    "])\n",
    "\n",
    "# Apply transformation to the dataset\n",
    "def transform_dataset(batch):\n",
    "    batch['image'] = [transform(image) for image in batch['image']]\n",
    "    return batch\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx]['image']\n",
    "        label = self.dataset[idx]['label']\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 👇 The important part\n",
    "def mlx_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = mx.array([mx.array(image.numpy()) for image in images])\n",
    "    labels = mx.array(labels)\n",
    "    return images, labels\n",
    "\n",
    "# Create DataLoaders with the custom collate function\n",
    "def create_dataloader(dataset, transform=None, batch_size=32, shuffle=False):\n",
    "    return DataLoader(MNISTDataset(dataset, transform=transform), # 👈 here we pass the transform we defined earlier\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      collate_fn=mlx_collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "ds_name = \"ylecun/mnist\"\n",
    "ds = load_dataset(ds_name)\n",
    "ds = ds['train'].train_test_split(test_size=0.2)\n",
    "ds['val'] = ds['test']\n",
    "del ds['test']\n",
    "\n",
    "train_loader = create_dataloader(ds['train'], transform=transform, batch_size=batch_size, shuffle=True)\n",
    "val_loader = create_dataloader(ds['val'], transform=transform, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for inputs, labels in train_loader:\n",
    "    print('inputs:', type(inputs), inputs.shape)\n",
    "    print('labels:', type(labels), labels.shape)\n",
    "   \n",
    "    print('The image should display a ', labels[0].item())\n",
    "    np_show_img(inputs[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! You are now ready to fed these mxarrays to your mx neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch time: 8.70416 (s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tic = time.perf_counter()\n",
    "for inputs, labels in train_loader:\n",
    "    continue\n",
    "toc = time.perf_counter()\n",
    "print(f\"epoch time: {toc - tic:.5f} (s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
