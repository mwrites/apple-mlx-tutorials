{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Hugging Face Dataset to PyTorch DataLoaders and MX Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide walks you through loading a dataset with Hugging Face, batching and collating the data using PyTorchâ€™s DataLoader, and converting the tensors to MX arrays for neural network input, here are the steps:\n",
    "\n",
    "0. Why Not `mlx.data` ?\n",
    "1. Load Dataset from Hugging Face\n",
    "2. Define Your Transforms\n",
    "3. Leverage PyTorch DataLoaders with a Custom Collate Function\n",
    "4. Verify the Setup\n",
    "5. Full Snippet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Install PyTorch if needed](https://pytorch.org/get-started/locally/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Hugging Face Datasets Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlx\n",
    "import mlx.core as mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Why Not Use `mlx.data`?\n",
    "\n",
    "While using PyTorch utilities does introduce a bit of dependency overload, we don't have to reinvent the wheel and we can stick to the usual industry workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use HuggingFace's `datasets` library to load the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_name = \"ylecun/mnist\"\n",
    "ds = load_dataset(ds_name)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weâ€™ll use only the training set for now, splitting it into two parts:\n",
    "- Training Set: 80% for training our model.\n",
    "- Validation Set: 20% for checking the modelâ€™s accuracy at each epoch.\n",
    "\n",
    "Weâ€™ll keep the test set aside for the final evaluation to see how our model performs on completely unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 48000\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "# just some renaming to stay consistent with what I just said\n",
    "ds['val'] = ds['test']\n",
    "del ds['test']\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iuhtvBesXXgq88WLHGmmWsqxMZGKs5JC5QYwQCQDz1P1rnqK6/4d+BLrx14gFsCYdOt8SXlz2jT0B/vHnH4ntWz8U/H0evXMfh7QtsHhrTSI4I412iVlGN3+76fn3483p8UZmmSNersFH416z461yDwJ4cj+Hfh2QeZ5e/WbsD5pZWAJQHjjHHTptHrXkdFOR2jdXRirqcqynBB9RSzTS3M8k88jyzSMXeR2LMzE5JJPUk96ZRX/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAoklEQVR4AWNgGAIg+X8VTldy7/z3UQ6X7LK//17ikGNZ9/vvl1Acki1///6di0NO9f2/L7jkOHb9/bsChz7eif/+PtHGIZn9998vTxxytj/+/qvEIaf84N+/I/w4JE/8/Xsel1zXx7+X3HDok//390MyLrn3/95q4JATrv/3fzoOOQatL3+PsOGSNP9+DzMOmaCqOX+cfoRLI0PqVZxSVJcAAKG/P/OyJPcCAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a sample image and label from the training set\n",
    "sample = ds['train'][0]\n",
    "sample['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the above image is supposed to be a 1\n"
     ]
    }
   ],
   "source": [
    "print('the above image is supposed to be a', sample['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define Your Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we got a PILImage which is not useful for our model, so we need to define a `transform` to go from PIL To Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([784]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    lambda x: x.view(-1)  # Flatten to 1D array of 784 pixels\n",
    "])\n",
    "\n",
    "# Apply transformation to the dataset\n",
    "def transform_dataset(batch):\n",
    "    batch['image'] = [transform(image) for image in batch['image']]\n",
    "    return batch\n",
    "\n",
    "# Apply the transformation to the dataset\n",
    "transformed = ds.with_transform(transform_dataset)\n",
    "\n",
    "# Example of accessing transformed data\n",
    "type(transformed['train'][0]['image']), transformed['train'][0]['image'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that this part `lambda x: x.view(-1)` may vary depending on the requirements of your neural network. A flat tensor is suitable for a multi-layer perceptron, but for a convolutional network, you might need a 3-dimensional tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that we can still display the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# helper to display an image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def np_show_img(array, title=''):\n",
    "    if array.ndim == 1:  # If rank 1, reshape to 2D\n",
    "        array = array.reshape(28, 28)\n",
    "    plt.imshow(array, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "np_show_img(transformed['train'][0]['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Leverage PyTorch DataLoaders with a Custom Collate Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we successfully applied the transform, but in practice we don't apply transforms on the dataset all at once. We do that in batches, load the stuff in parallel, cache the results, etc.\n",
    "\n",
    "Let's not bother rewriting all this stuff!\n",
    "\n",
    "Instead we can leverage `DataLoader` and `Dataset` classes from the PyTorch utility library and apply a custom collate function to collect our mlx arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import mlx.core as mx\n",
    "\n",
    "# Custom Dataset class to match what the data structure we got from hugging face\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx]['image']\n",
    "        label = self.dataset[idx]['label']\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# ðŸ‘‡ The important part\n",
    "def mlx_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = mx.array([mx.array(image.numpy()) for image in images])\n",
    "    labels = mx.array(labels)\n",
    "    return images, labels\n",
    "\n",
    "# Create DataLoaders with the custom collate function\n",
    "def create_dataloader(dataset, transform=None, batch_size=32, shuffle=False):\n",
    "    return DataLoader(MNISTDataset(dataset, transform=transform), # ðŸ‘ˆ here we pass the transform we defined earlier\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      collate_fn=mlx_collate_fn)\n",
    "\n",
    "\n",
    "train_loader = create_dataloader(ds['train'], transform=transform, batch_size=batch_size, shuffle=True)\n",
    "val_loader = create_dataloader(ds['val'], batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Verify the Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: <class 'mlx.core.array'> (32, 784)\n",
      "labels: <class 'mlx.core.array'> (32,)\n",
      "The image should display a  8\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for inputs, labels in train_loader:\n",
    "    print('inputs:', type(inputs), inputs.shape)\n",
    "    print('labels:', type(labels), labels.shape)\n",
    "   \n",
    "    print('The image should display a ', labels[0].item())\n",
    "    np_show_img(inputs[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Full Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: <class 'mlx.core.array'> (32, 784)\n",
      "labels: <class 'mlx.core.array'> (32,)\n",
      "The image should display a  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import mlx\n",
    "import mlx.core as mx\n",
    "\n",
    "# just a helper to show img\n",
    "def np_show_img(array, title=''):\n",
    "    if array.ndim == 1:  # If rank 1, reshape to 2D\n",
    "        array = array.reshape(28, 28)\n",
    "    plt.imshow(array, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    lambda x: x.view(-1)  # Flatten to 1D array of 784 pixels\n",
    "])\n",
    "\n",
    "# Apply transformation to the dataset\n",
    "def transform_dataset(batch):\n",
    "    batch['image'] = [transform(image) for image in batch['image']]\n",
    "    return batch\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx]['image']\n",
    "        label = self.dataset[idx]['label']\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# ðŸ‘‡ The important part\n",
    "def mlx_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = mx.array([mx.array(image.numpy()) for image in images])\n",
    "    labels = mx.array(labels)\n",
    "    return images, labels\n",
    "\n",
    "# Create DataLoaders with the custom collate function\n",
    "def create_dataloader(dataset, transform=None, batch_size=32, shuffle=False):\n",
    "    return DataLoader(MNISTDataset(dataset, transform=transform), # ðŸ‘ˆ here we pass the transform we defined earlier\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      collate_fn=mlx_collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "ds_name = \"ylecun/mnist\"\n",
    "ds = load_dataset(ds_name)\n",
    "ds = ds['train'].train_test_split(test_size=0.2)\n",
    "ds['val'] = ds['test']\n",
    "del ds['test']\n",
    "\n",
    "train_loader = create_dataloader(ds['train'], transform=transform, batch_size=batch_size, shuffle=True)\n",
    "val_loader = create_dataloader(ds['val'], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for inputs, labels in train_loader:\n",
    "    print('inputs:', type(inputs), inputs.shape)\n",
    "    print('labels:', type(labels), labels.shape)\n",
    "   \n",
    "    print('The image should display a ', labels[0].item())\n",
    "    np_show_img(inputs[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! You are now ready to fed these mxarrays to your mx neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
